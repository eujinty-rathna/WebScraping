{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab848daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing required modules here'''\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from price_parser import Price\n",
    "\n",
    "\n",
    "class HipStore:\n",
    "    \n",
    "    '''this class is about to scrape the hipstore website'''\n",
    "    \n",
    "    def __init__(self,url,headers,cookies):\n",
    "        \n",
    "        '''initilizing class attributes here'''\n",
    "        \n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        self.cookies = cookies\n",
    "        self.product_urls = []\n",
    "        self.product_dic = {}\n",
    "        self.product_list = []\n",
    "        self.crawl()\n",
    "        \n",
    "    def start_requst(self):\n",
    "        \n",
    "        ''' this method is to send the initial request to website\n",
    "        and to return the category links'''\n",
    "        \n",
    "        response = requests.get(self.url,headers=self.headers,cookies=self.cookies)\n",
    "        soup  = BeautifulSoup(response.text,'html.parser')\n",
    "        category_links = [urljoin(self.url,link['href']) for link in soup.find_all('a',href=re.compile('w?o?mens?/footwear/'))]\n",
    "        return category_links\n",
    "    \n",
    "        \n",
    "    def get_product_urls(self,category_urls):\n",
    "        \n",
    "        '''using this function we are getting all the product urls \n",
    "        from all the pages of category urls that we got from previous func'''\n",
    "        \n",
    "        # this function helps us to navigate to each of the pages\n",
    "        def pagination(soup):\n",
    "            next_button = soup.find('a',{'rel':'next','title':'Next Page'})\n",
    "            if next_button is not None:\n",
    "                print(f'paginating..to this page{next_button[\"href\"]}')\n",
    "                self.get_product_urls([urljoin(self.url,next_button['href'])])\n",
    "                \n",
    "        # we getting the product urls here\n",
    "        for url in category_urls:\n",
    "            response = requests.get(url,headers=self.headers,cookies=self.cookies)\n",
    "            soup = BeautifulSoup(response.text,'lxml')\n",
    "            product_urls =[urljoin(self.url,link['href']) for link in  soup.find_all('a',class_=\"itemImage\")]\n",
    "            self.product_urls = self.product_urls+product_urls\n",
    "            \n",
    "            pagination(soup) # calling pagination function\n",
    "\n",
    "        return list(set(self.product_urls)) # we are filtering the urls here and return all the urls.\n",
    "    \n",
    "    def parse_details(self, url_list):\n",
    "        \n",
    "        '''this function is about to parse the product details like\n",
    "        name , brand , price size , color.. and  so on '''\n",
    "        \n",
    "        # to get the currency symbol\n",
    "        def get_symbol(price_string):\n",
    "            symbol = Price.fromstring(price_string).currency\n",
    "            return symbol\n",
    "        \n",
    "        # to get price value as float \n",
    "        def get_amount(price_string):\n",
    "            amount  = Price.fromstring(price_string).amount_float\n",
    "            return amount\n",
    "\n",
    "        \n",
    "        for url in url_list:\n",
    "\n",
    "            response= requests.get(url,headers=headers,cookies=cookies)\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            product_urls = url\n",
    "            try:\n",
    "                brand = soup.find('div',class_='pdp-title').h1.text.strip()\n",
    "            except:\n",
    "                brand = ''\n",
    "            try:\n",
    "                name = soup.find('div',class_='pdp-title').h2.text.strip()\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                price = soup.find('div',class_='itemPrices').span.text.strip()\n",
    "                symbol = get_symbol(price) #getting symbol of the currency\n",
    "                price = get_amount(price) # getting float of the price  value\n",
    "            except:\n",
    "                price = ''\n",
    "                symbol  = ''\n",
    "            try:\n",
    "                colors = ','.join([li.text.strip() for li in soup.find('ul',{'class':'productSelectDropDown'}).find_all('li')])\n",
    "            except :\n",
    "                colors = ''\n",
    "            try:\n",
    "                sizes = [button.text.strip() for button in soup.find('div',{\n",
    "                    'class':'productSelectDropDown options'}).find_all('button')]\n",
    "                size_numbers = ','.join([re.search(r'\\d\\d?\\.?\\d?\\d?',size).group() for size in sizes])  # getting size from the country\n",
    "                size_country =','.join(list(set([re.search(r'[A-z]+',size).group() for size in sizes]))) # getting country from the size\n",
    "                \n",
    "            except :\n",
    "                sizes = ''\n",
    "                size_numbers = ''\n",
    "                size_country = ''\n",
    "\n",
    "            #updating dictionary\n",
    "            dic = {'product_url':url,\n",
    "                   'product_brand':brand,\n",
    "                   'product_name':name,\n",
    "                   'product_price':price,\n",
    "                   'currency_symbol':symbol,\n",
    "                   'product_sizes':size_numbers,\n",
    "                   'size_country':size_country,\n",
    "                   'product_colors':colors,\n",
    "                   'product_price':price,\n",
    "            }\n",
    "\n",
    "            # appending all the products detail to list\n",
    "            self.product_list.append(dic)\n",
    "\n",
    "            print('--'*50)\n",
    "            print(' '*200)\n",
    "            print(f'{url}\\n{name}\\n{price}\\n{brand}\\n{sizes}\\n{colors}')\n",
    "            print(dic)\n",
    "            print(' '*200)\n",
    "            print('--'*50)\n",
    "\n",
    "    def save_csv(self):\n",
    "        df = pd.DataFrame(self.product_list)\n",
    "        csv = df.to_csv('hipstore.csv',index=False)\n",
    "        \n",
    "    def crawl(self):\n",
    "        \n",
    "        ''' this function is about to call all the function and \n",
    "        to manage the crawling process'''\n",
    "        \n",
    "        category_urls = self.start_requst()\n",
    "        product_urls = self.get_product_urls(category_urls)\n",
    "        self.parse_details(product_urls)\n",
    "        self.save_csv()\n",
    "        \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79dbd00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "    url = 'https://www.thehipstore.co.uk/'\n",
    "    cookies = {\n",
    "        '49746': '',\n",
    "        '__rtbh.lid': '%7B%22eventType%22%3A%22lid%22%2C%22id%22%3A%22qM7042gQOSYJhZCReLNe%22%7D',\n",
    "        'language': 'en',\n",
    "        'gdprsettings2': '{\"functional\":true,\"performance\":true,\"targeting\":true}',\n",
    "        'gdprsettings3': '{\"functional\":true,\"performance\":true,\"targeting\":true}',\n",
    "        '_gcl_au': '1.1.1188987379.1681101388',\n",
    "        '_tt_enable_cookie': '1',\n",
    "        '_ttp': 'RADpLp1b4Zrhg-2NIh02BjM3StG',\n",
    "        '_fbp': 'fb.2.1681101388835.1026343447',\n",
    "        'mt.v': '2.1664255124.1681101388996',\n",
    "        '__pr.1psq': 'h2h5OHFGVJ',\n",
    "        '_taggstar_vid': '4d767c72-d759-11ed-aebc-f13604e20454',\n",
    "        '_gid': 'GA1.3.2027253970.1681201619',\n",
    "        '_uetsid': 'a0ffb8e0d84211ed8e097fdb54efd665',\n",
    "        '_uetvid': '42144b80d75911ed9cc3b733819a4335',\n",
    "        '_ga': 'GA1.3.857185955.1681101377',\n",
    "        'cto_bundle': 'fvhHKl9ZYTF6JTJCYTVzOUdmclNJeDhUUHF2M2NPS05lTHhPbDN5UVVqVTNJVFl3ZTQwZU1XcSUyQjZlektOREtQUCUyRjNNJTJGVDZlT3N5SWFTVjNZMEREdjdocTd2MEFoTTRqVVA2cTFldlk5VU52cFZqZE5ReTVwdVR4aHdxOW5JOVUzYlZIak92VjhUbzJyZ2ZQRFY2WnljTmVrQUNaV3R3Tmk0UnpBZXRRdE9XbVBXanhkQSUzRA',\n",
    "        'redwp': '_',\n",
    "        'hero-session-94356483-975d-44d9-a4cb-2cd3f7e7e0f9': 'author=client&expires=1712737627393&visitor=600ca114-8a3d-4797-90a3-f2fbd645511d',\n",
    "        '_ga_MKDXQSME2D': 'GS1.1.1681201619.2.0.1681201627.0.0.0',\n",
    "        'akavpau_VP1': '1681201948~id=fb3bcd0ad055094e4e33d3c51f3dad00',\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'www.thehipstore.co.uk',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'accept-language': 'en-US,en;q=0.9,ta;q=0.8,hi;q=0.7',\n",
    "        'cache-control': 'no-cache',\n",
    "        # 'cookie': '49746=; __rtbh.lid=%7B%22eventType%22%3A%22lid%22%2C%22id%22%3A%22qM7042gQOSYJhZCReLNe%22%7D; language=en; gdprsettings2={\"functional\":true,\"performance\":true,\"targeting\":true}; gdprsettings3={\"functional\":true,\"performance\":true,\"targeting\":true}; _gcl_au=1.1.1188987379.1681101388; _tt_enable_cookie=1; _ttp=RADpLp1b4Zrhg-2NIh02BjM3StG; _fbp=fb.2.1681101388835.1026343447; mt.v=2.1664255124.1681101388996; __pr.1psq=h2h5OHFGVJ; _taggstar_vid=4d767c72-d759-11ed-aebc-f13604e20454; _gid=GA1.3.2027253970.1681201619; _uetsid=a0ffb8e0d84211ed8e097fdb54efd665; _uetvid=42144b80d75911ed9cc3b733819a4335; _ga=GA1.3.857185955.1681101377; cto_bundle=fvhHKl9ZYTF6JTJCYTVzOUdmclNJeDhUUHF2M2NPS05lTHhPbDN5UVVqVTNJVFl3ZTQwZU1XcSUyQjZlektOREtQUCUyRjNNJTJGVDZlT3N5SWFTVjNZMEREdjdocTd2MEFoTTRqVVA2cTFldlk5VU52cFZqZE5ReTVwdVR4aHdxOW5JOVUzYlZIak92VjhUbzJyZ2ZQRFY2WnljTmVrQUNaV3R3Tmk0UnpBZXRRdE9XbVBXanhkQSUzRA; redwp=_; hero-session-94356483-975d-44d9-a4cb-2cd3f7e7e0f9=author=client&expires=1712737627393&visitor=600ca114-8a3d-4797-90a3-f2fbd645511d; _ga_MKDXQSME2D=GS1.1.1681201619.2.0.1681201627.0.0.0; akavpau_VP1=1681201948~id=fb3bcd0ad055094e4e33d3c51f3dad00',\n",
    "        'dnt': '1',\n",
    "        'pragma': 'no-cache',\n",
    "        'referer': 'https://www.reddit.com/',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    hipstore = HipStore(url,headers,cookies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bda4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
